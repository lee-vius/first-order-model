{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74bfacd-dba9-4433-b8a5-3910cb8b0725",
   "metadata": {},
   "source": [
    "# Original FOMM Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43e23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages needed for demonstration\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from skimage.transform import resize\n",
    "from IPython.display import HTML\n",
    "\n",
    "from demo import make_animation\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acda722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the source video and target image\n",
    "target_path = \"raw_data/targets/3.png\"\n",
    "source_path = \"raw_data/sources/00048.mp4\"\n",
    "\n",
    "source_image = imageio.imread(target_path)\n",
    "reader = imageio.get_reader(source_path)\n",
    "\n",
    "# pre process the video and image\n",
    "source_image = resize(source_image, (256, 256))[..., :3]\n",
    "fps = reader.get_meta_data()['fps'] # number of frames\n",
    "\n",
    "# Add each frame of the video\n",
    "driving_video = []\n",
    "try:\n",
    "    for im in reader:\n",
    "        driving_video.append(im)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "reader.close()\n",
    "# resize each frame in the video to 256x256\n",
    "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad47fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that generates a piece of video\n",
    "def display(source, driving, generated=None):\n",
    "    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n",
    "\n",
    "    ims = []\n",
    "    for i in range(len(driving)):\n",
    "        cols = [source]\n",
    "        cols.append(driving[i])\n",
    "        if generated is not None:\n",
    "            cols.append(generated[i])\n",
    "        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n",
    "        plt.axis('off')\n",
    "        ims.append([im])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
    "    plt.close()\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e79bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the deep network\n",
    "from demo import load_checkpoints\n",
    "generator, kp_detector = load_checkpoints(config_path='config/vox-256.yaml', \n",
    "                            checkpoint_path='pre_trains/vox-cpk.pth.tar', cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2bb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate animation\n",
    "predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True, cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the video\n",
    "HTML(display(source_image, driving_video, predictions).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa637c21-d35b-4185-8544-35feee263bf8",
   "metadata": {},
   "source": [
    "# Stylizer Added Demo\n",
    "Now we add the expression stylizer to see the effects transferred to expressions of animated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b18f53-a44a-4429-86cc-36f14d478b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages needed for demonstration\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from skimage.transform import resize\n",
    "from IPython.display import HTML\n",
    "\n",
    "from demo import make_animation\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9f503a-153f-41fa-8b70-3c276b8135d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the source video and target image\n",
    "target_path = \"raw_data/targets/3.png\"\n",
    "source_path = \"raw_data/sources/00048.mp4\"\n",
    "\n",
    "source_image = imageio.imread(target_path)\n",
    "reader = imageio.get_reader(source_path)\n",
    "\n",
    "# pre process the video and image\n",
    "source_image = resize(source_image, (256, 256))[..., :3]\n",
    "fps = reader.get_meta_data()['fps'] # number of frames\n",
    "\n",
    "# Add each frame of the video\n",
    "driving_video = []\n",
    "try:\n",
    "    for im in reader:\n",
    "        driving_video.append(im)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "reader.close()\n",
    "# resize each frame in the video to 256x256\n",
    "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05573db-1f07-47c2-ab31-e74caa08f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that generates a piece of video\n",
    "def display(source, driving, generated=None):\n",
    "    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n",
    "\n",
    "    ims = []\n",
    "    for i in range(len(driving)):\n",
    "        cols = [source]\n",
    "        cols.append(driving[i])\n",
    "        if generated is not None:\n",
    "            cols.append(generated[i])\n",
    "        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n",
    "        plt.axis('off')\n",
    "        ims.append([im])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
    "    plt.close()\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f7b0eb-0294-4514-94e7-17967ed32845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KPDetector(\n",
       "  (predictor): Hourglass(\n",
       "    (encoder): Encoder(\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): DownBlock2d(\n",
       "          (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "        )\n",
       "        (1): DownBlock2d(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "        )\n",
       "        (2): DownBlock2d(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "        )\n",
       "        (3): DownBlock2d(\n",
       "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "        )\n",
       "        (4): DownBlock2d(\n",
       "          (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (up_blocks): ModuleList(\n",
       "        (0): UpBlock2d(\n",
       "          (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): UpBlock2d(\n",
       "          (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): UpBlock2d(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): UpBlock2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): UpBlock2d(\n",
       "          (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): SynchronizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (kp): Conv2d(35, 10, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (jacobian): Conv2d(35, 40, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (down): AntiAliasInterpolation2d()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator\n",
    "from modules.keypoint_detector import KPDetector\n",
    "from animate import normalize_kp\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "checkpoint_path = \"pre_trains/vox-cpk.pth.tar\"\n",
    "config_path='config/anim-256.yaml'\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "# initialize generator\n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "# initialize kp detector\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "\n",
    "# If GPU Available, adapt to it\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using GPU\")\n",
    "    generator.to(0)\n",
    "    kp_detector.to(0)\n",
    "    \n",
    "# load in the pretrained modules\n",
    "train_params = config['train_params']\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # remember to adapt to cpu version\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "generator.load_state_dict(checkpoint['generator'])\n",
    "kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "\n",
    "# The following models are used as data pre-processor\n",
    "generator.eval()\n",
    "kp_detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b511b-0f28-4c39-a4aa-cd1fc791ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the stylizer we need\n",
    "# declare objects needed by training process\n",
    "import torch\n",
    "from modules.stylizer import StylizerGenerator\n",
    "from modules.stylizer_discriminator import StylizerDiscrim\n",
    "\n",
    "# create network models\n",
    "stylizer = StylizerGenerator(**config['model_params']['stylizer_params'])\n",
    "styDiscrim = StylizerDiscrim(**config['model_params']['stylizerDiscrim_params'])\n",
    "\n",
    "# If GPU Available, adapt to it\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using GPU\")\n",
    "    stylizer.to(0)\n",
    "    styDiscrim.to(0)\n",
    "    \n",
    "# load in pretrained modules\n",
    "stylizer_checkpoint = \"pre_trains/00000099-checkpoint.pth.tar\"\n",
    "stylizer_checkpoint = torch.load(stylizer_checkpoint)\n",
    "stylizer.load_state_dict(checkpoint['stylizer'])\n",
    "styDiscrim.load_state_dict(checkpoint['styDiscrim'])\n",
    "# set to evaluate mode\n",
    "stylizer.eval()\n",
    "styDiscrim.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12872f-81f0-4be3-8467-dec14002aca4",
   "metadata": {},
   "source": [
    "## define funtion of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7babd5b-9153-42c1-88c6-bfc23726d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With modules given, generate final results\n",
    "from animate import normalize_kp\n",
    "\n",
    "def my_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        # turn source and driving to tensor\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        # pass in the detector for a result\n",
    "        kp_source = kp_detector(source)\n",
    "        kp_driving_initial = kp_detector(driving[:, :, 0])\n",
    "\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            kp_driving = kp_detector(driving_frame)\n",
    "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
    "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
    "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
    "            # ---------------------------------------- #\n",
    "            # TODO: replace the generator below\n",
    "            dm_network = generator.dense_motion_network\n",
    "            out = generator.first(source)\n",
    "            for i in range(len(generator.down_blocks)):\n",
    "                out = generator.down_blocks[i](out)\n",
    "\n",
    "            # Transforming feature representation according to deformation and occlusion\n",
    "            # 通过形变等信息来变换特征向量\n",
    "            output_dict = {}\n",
    "            if dm_network is not None:\n",
    "                # 通过 稠密运动网络模块 获取运动变换信息\n",
    "                # ------------------------------------------ #\n",
    "                # TODO: replace dense motion\n",
    "                if dm_network.scale_factor != 1:\n",
    "                    src_image = dm_network.down(source)\n",
    "                \n",
    "                bs, _, h, w = src_image.shape\n",
    "\n",
    "                dm_out_dict = dict()\n",
    "                heatmap_representation = dm_network.create_heatmap_representations(src_image, kp_driving, kp_source)\n",
    "                orig_sparse_motion = dm_network.create_sparse_motion(src_image, kp_driving, kp_source)\n",
    "                sparse_motion = orig_sparse_motion[:, :-1, :, :, :] # The shape should be (1, kp_num, 64, 64, 2)\n",
    "                temp_shape = sparse_motion.shape\n",
    "                sparse_motion = sparse_motion.permute((0, 1, 4, 2, 3)).reshape((temp_shape[0], temp_shape[1] * temp_shape[4], temp_shape[2], temp_shape[3]))\n",
    "                # now the shape is (1, kp_num * 2, 64, 64)\n",
    "                stylized_motion = stylizer(sparse_motion)['prediction'] # this is the stylized sparse motion\n",
    "                stylized_motion = stylized_motion.reshape((temp_shape[0], temp_shape[1], temp_shape[4], temp_shape[2], temp_shape[3])).permute((0, 1, 3, 4, 2))\n",
    "                orig_sparse_motion[:, :-1, :, :, :] = stylized_motion\n",
    "                # now the shape is (1, kp_num, 64, 64, 2), which is the component we want\n",
    "                deformed_source = dm_network.create_deformed_source_image(src_image, orig_sparse_motion)\n",
    "                dm_out_dict['sparse_deformed'] = deformed_source\n",
    "                \n",
    "                input = torch.cat([heatmap_representation, deformed_source], dim=2)\n",
    "                input = input.view(bs, -1, h, w)\n",
    "                prediction = dm_network.hourglass(input)\n",
    "                \n",
    "                mask = dm_network.mask(prediction)\n",
    "                mask = F.softmax(mask, dim=1)\n",
    "                dm_out_dict['mask'] = mask\n",
    "                \n",
    "                mask = mask.unsqueeze(2)\n",
    "                orig_sparse_motion = orig_sparse_motion.permute(0, 1, 4, 2, 3)\n",
    "                deformation = (orig_sparse_motion * mask).sum(dim=1)\n",
    "                deformation = deformation.permute(0, 2, 3, 1)\n",
    "                \n",
    "                dm_out_dict['deformation'] = deformation\n",
    "                \n",
    "                # Sec. 3.2 in the paper\n",
    "                if dm_network.occlusion:\n",
    "                    occlusion_map = torch.sigmoid(dm_network.occlusion(prediction))\n",
    "                    dm_out_dict['occlusion_map'] = occlusion_map\n",
    "                \n",
    "                # ------------------------------------------ #\n",
    "                # back to generator\n",
    "                output_dict['mask'] = dm_out_dict['mask']\n",
    "                output_dict['sparse_deformed'] = dm_out_dict['sparse_deformed']\n",
    "\n",
    "                if 'occlusion_map' in dm_out_dict:\n",
    "                    occlusion_map = dm_out_dict['occlusion_map']\n",
    "                    output_dict['occlusion_map'] = occlusion_map\n",
    "                else:\n",
    "                    occlusion_map = None\n",
    "                deformation = dm_out_dict['deformation']\n",
    "                # 最终在此步对 encode 出来的特征值进行变换\n",
    "                out = generator.deform_input(out, deformation)\n",
    "\n",
    "                if occlusion_map is not None:\n",
    "                    if out.shape[2] != occlusion_map.shape[2] or out.shape[3] != occlusion_map.shape[3]:\n",
    "                        occlusion_map = F.interpolate(occlusion_map, size=out.shape[2:], mode='bilinear')\n",
    "                    out = out * occlusion_map\n",
    "\n",
    "                output_dict[\"deformed\"] = generator.deform_input(source, deformation)\n",
    "\n",
    "            # Decoding part\n",
    "            out = generator.bottleneck(out)\n",
    "            for i in range(len(generator.up_blocks)):\n",
    "                out = generator.up_blocks[i](out)\n",
    "            out = generator.final(out)\n",
    "            out = F.sigmoid(out)\n",
    "\n",
    "            output_dict[\"prediction\"] = out\n",
    "            # -------------------------------- End of generator ----------------------------------# \n",
    "            predictions.append(np.transpose(output_dict['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97769d-e0db-4ae4-b2a8-a39d65aa5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate animation\n",
    "predictions = my_animation(source_image, driving_video, generator, kp_detector, relative=True, cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ca72b-7e74-4c92-96a5-88d9f73ada0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the video\n",
    "HTML(display(source_image, driving_video, predictions).to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
