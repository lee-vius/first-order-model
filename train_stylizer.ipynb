{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565f48b0",
   "metadata": {},
   "source": [
    "# Training for Stylizer module\n",
    "This notebook will handle the training the stylizer module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76182036",
   "metadata": {},
   "source": [
    "## Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce87f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from time import gmtime, strftime\n",
    "from shutil import copy\n",
    "\n",
    "from frames_dataset import FramesDataset\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator # LCH: refer here for generator\n",
    "from modules.discriminator import MultiScaleDiscriminator # LCH: refer here for discriminator\n",
    "from modules.keypoint_detector import KPDetector # LCH: refer here for key point detector\n",
    "\n",
    "import torch\n",
    "\n",
    "from train import train # LCH: For training process, everything in this module\n",
    "from reconstruction import reconstruction\n",
    "from animate import animate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18466946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\59714\\Anaconda3\\envs\\firOrder\\lib\\site-packages\\ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "config_path = \"config/anim-256.yaml\"\n",
    "with open(config_path) as f:\n",
    "        # read in the config file\n",
    "        config = yaml.load(f) # config file contains code directions, including training details\n",
    "\n",
    "checkpoint_path = \"pre_trains/vox-cpk.pth.tar\"\n",
    "log_dir = \"MyLog/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "# Copy the config file (*.yaml) into the logging path\n",
    "if not os.path.exists(os.path.join(log_dir, os.path.basename(config_path))):\n",
    "    copy(config_path, log_dir)\n",
    "\n",
    "# initialize generator\n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "# initialize discriminator\n",
    "discriminator = MultiScaleDiscriminator(**config['model_params']['discriminator_params'],\n",
    "                                            **config['model_params']['common_params'])\n",
    "# initialize kp detector\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "\n",
    "# If GPU Available, adapt to it\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"using GPU\")\n",
    "#     generator.to(0)\n",
    "#     discriminator.to(0)\n",
    "#     kp_detector.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef3da41-9020-4ad1-a0cf-98a263a7de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\59714\\Anaconda3\\envs\\firOrder\\lib\\site-packages\\torch\\cuda\\__init__.py:117: UserWarning: \n",
      "    Found GPU0 GeForce GTX 770 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "# load in the pretrained modules\n",
    "from logger import Logger\n",
    "\n",
    "train_params = config['train_params']\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # remember to adapt to cpu version\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "generator.load_state_dict(checkpoint['generator'])\n",
    "discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "kp_detector.load_state_dict(checkpoint['kp_detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c0bd7-5ec6-418c-8e9a-b76c52cf4859",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c3ca21-e18a-4060-98a9-6cab2aad3e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "Dataset size: 270, repeat number: 4\n",
      "Repeated Dataset size: 1080, repeat number: 4\n"
     ]
    }
   ],
   "source": [
    "from frames_dataset import DatasetRepeater\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# load original target data\n",
    "frame_dataset = FramesDataset(is_train=True, **config['dataset_params'])\n",
    "print(\"Dataset size: {}, repeat number: {}\".format(len(frame_dataset), config['train_params']['num_repeats']))\n",
    "\n",
    "if 'num_repeats' in train_params or train_params['num_repeats'] != 1:\n",
    "    # Augment the dataset according to \"num_reapeat\"\n",
    "    frame_dataset = DatasetRepeater(frame_dataset, train_params['num_repeats'])\n",
    "    print(\"Repeated Dataset size: {}, repeat number: {}\".format(len(frame_dataset), config['train_params']['num_repeats']))\n",
    "\n",
    "dataloader = DataLoader(frame_dataset, batch_size=train_params['batch_size'], shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b0740d-b207-4a7b-8de5-ddd997aa5225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 256, 256])\n",
      "source image shape: torch.Size([10, 3, 64, 64])\n",
      "sparse motion shape: torch.Size([10, 10, 64, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "# get sparse motion field by the original models\n",
    "# check the dense motion module\n",
    "if generator.dense_motion_network is None:\n",
    "    print(\"Error: dense motion network doesn't exist!\")\n",
    "dm_network = generator.dense_motion_network\n",
    "\n",
    "# data fetching schema\n",
    "for x in dataloader:\n",
    "    print(x['source'].shape)\n",
    "    \n",
    "    # first get the key points for both source and driving\n",
    "    kp_source = kp_detector(x['source'])\n",
    "    kp_driving = kp_detector(x['driving'])\n",
    "    \n",
    "    # second pass through the motion predictor\n",
    "    # plan A: get sparse motion as training data\n",
    "    if dm_network.scale_factor != 1:\n",
    "        src_image = dm_network.down(x['source'])\n",
    "    \n",
    "    bs, _, h, w = src_image.shape\n",
    "    print(\"source image shape: {}\".format(src_image.shape))\n",
    "    sparse_motion = dm_network.create_sparse_motions(src_image, kp_driving, kp_source)\n",
    "    # here we don't need the last key point, which is a identity grid layer added by users\n",
    "    sparse_motion = sparse_motion[:, :-1, :, :, :]\n",
    "    print(\"sparse motion shape: {}\".format(sparse_motion.shape)) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0582b411-9176-4a63-946a-2fc20decafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_motion = sparse_motion.permute(0, 1, 3, 4, 2)\n",
    "s = sparse_motion.shape\n",
    "# sparse_motion.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b90d02d6-d660-4ce8-a18b-051d308ca016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 64, 64, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 64, 64, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s)\n",
    "sparse_motion.view((s[0], -1, s[2], s[3])).view(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c22bd-4607-4d95-a90f-d6706529bdb1",
   "metadata": {},
   "source": [
    "## Code snippets part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f814d334-3a9a-4ebb-a67f-ca0a1e27b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4, 4, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, w = 4, 4\n",
    "x = torch.arange(w).type(torch.float)\n",
    "y = torch.arange(h).type(torch.float)\n",
    "\n",
    "x = (2 * (x / (w - 1)) - 1)\n",
    "y = (2 * (y / (h - 1)) - 1)\n",
    "\n",
    "yy = y.view(-1, 1).repeat(1, w)\n",
    "xx = x.view(1, -1).repeat(h, 1)\n",
    "\n",
    "meshed = torch.cat([xx.unsqueeze_(2), yy.unsqueeze_(2)], 2)\n",
    "\n",
    "print(meshed.shape)\n",
    "\n",
    "meshed = meshed.view(1, 1, h, w, 2)\n",
    "meshed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ebb3c-945a-4546-9ca9-e4d9825068ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
