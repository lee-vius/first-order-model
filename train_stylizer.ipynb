{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565f48b0",
   "metadata": {},
   "source": [
    "# Training for Stylizer module\n",
    "This notebook will handle the training the stylizer module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76182036",
   "metadata": {},
   "source": [
    "## Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import os, sys\n",
    "import yaml\n",
    "from argparse import ArgumentParser\n",
    "from time import gmtime, strftime\n",
    "from shutil import copy\n",
    "\n",
    "from frames_dataset import FramesDataset\n",
    "from my_dataset import MyDataset\n",
    "\n",
    "from modules.generator import OcclusionAwareGenerator # LCH: refer here for generator\n",
    "from modules.discriminator import MultiScaleDiscriminator # LCH: refer here for discriminator\n",
    "from modules.keypoint_detector import KPDetector # LCH: refer here for key point detector\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from train import train # LCH: For training process, everything in this module\n",
    "from reconstruction import reconstruction\n",
    "from animate import animate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18466946",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config/anim-256.yaml\"\n",
    "with open(config_path) as f:\n",
    "        # read in the config file\n",
    "        config = yaml.load(f) # config file contains code directions, including training details\n",
    "\n",
    "checkpoint_path = \"pre_trains/vox-cpk.pth.tar\"\n",
    "log_dir = \"MyLog/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "# Copy the config file (*.yaml) into the logging path\n",
    "if not os.path.exists(os.path.join(log_dir, os.path.basename(config_path))):\n",
    "    copy(config_path, log_dir)\n",
    "\n",
    "# initialize generator\n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "# initialize discriminator\n",
    "discriminator = MultiScaleDiscriminator(**config['model_params']['discriminator_params'],\n",
    "                                            **config['model_params']['common_params'])\n",
    "# initialize kp detector\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "\n",
    "# If GPU Available, adapt to it\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"using GPU\")\n",
    "#     generator.to(0)\n",
    "#     discriminator.to(0)\n",
    "#     kp_detector.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef3da41-9020-4ad1-a0cf-98a263a7de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the pretrained modules\n",
    "from logger import Logger\n",
    "\n",
    "train_params = config['train_params']\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # remember to adapt to cpu version\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "generator.load_state_dict(checkpoint['generator'])\n",
    "discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "kp_detector.load_state_dict(checkpoint['kp_detector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c0bd7-5ec6-418c-8e9a-b76c52cf4859",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3ca21-e18a-4060-98a9-6cab2aad3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frames_dataset import DatasetRepeater\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# load original target data\n",
    "frame_dataset = FramesDataset(is_train=True, **config['dataset_params'])\n",
    "print(\"Dataset size: {}, repeat number: {}\".format(len(frame_dataset), config['train_params']['num_repeats']))\n",
    "# load source data\n",
    "source_dataset = FramesDataset(is_train=True, **config['target_params'])\n",
    "print(\"Dataset size: {}, repeat number: {}\".format(len(source_dataset), config['train_params']['num_repeats']))\n",
    "# load combined data\n",
    "combine_dataset = MyDataset(source_dir=config['dataset_params']['root_dir'], target_dir=config['target_params']['root_dir'],\n",
    "                            frame_shape=config['dataset_params']['frame_shape'], id_sampling=config['dataset_params']['id_sampling'],\n",
    "                            is_train=True, augmentation_params=config['dataset_params']['augmentation_params'])\n",
    "print(\"Dataset size: {}, repeat number: {}\".format(len(combine_dataset), config['train_params']['num_repeats']))\n",
    "\n",
    "\n",
    "if 'num_repeats' in train_params or train_params['num_repeats'] != 1:\n",
    "    # Augment the dataset according to \"num_reapeat\"\n",
    "    frame_dataset = DatasetRepeater(frame_dataset, train_params['num_repeats'])\n",
    "    print(\"Repeated Target size: {}, repeat number: {}\".format(len(frame_dataset), config['train_params']['num_repeats']))\n",
    "    source_dataset = DatasetRepeater(source_dataset, train_params['num_repeats'])\n",
    "    print(\"Repeated Source size: {}, repeat number: {}\".format(len(source_dataset), config['train_params']['num_repeats']))\n",
    "    combine_dataset = DatasetRepeater(combine_dataset, train_params['num_repeats'])\n",
    "    print(\"Repeated Combin size: {}, repeat number: {}\".format(len(combine_dataset), config['train_params']['num_repeats']))\n",
    "    \n",
    "\n",
    "targetLoader = DataLoader(frame_dataset, batch_size=train_params['batch_size'], shuffle=True, num_workers=2, drop_last=True)\n",
    "sourceLoader = DataLoader(source_dataset, batch_size=train_params['batch_size'], shuffle=True, num_workers=2, drop_last=True)\n",
    "combineLoader = DataLoader(combine_dataset, batch_size=train_params['batch_size'], shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03327a1a-44a9-4157-85ca-a22c165a4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a model\n",
    "# declare objects needed by training process\n",
    "from modules.stylizer import StylizerGenerator\n",
    "from modules.stylizer_discriminator import StylizerDiscrim\n",
    "\n",
    "# create network models\n",
    "stylizer = StylizerGenerator(**config['model_params']['stylizer_params'])\n",
    "styDiscrim = StylizerDiscrim(**config['model_params']['stylizerDiscrim_params'])\n",
    "\n",
    "# create optimizers\n",
    "lr_stylizer = 2.0e-4\n",
    "lr_styDiscrim = 2.0e-4\n",
    "optimizer_stylizer = torch.optim.Adam(stylizer.parameters(), lr=lr_stylizer, betas=(0.5, 0.999), weight_decay=1e-2)\n",
    "optimizer_styDiscrim = torch.optim.Adam(styDiscrim.parameters(), lr=lr_styDiscrim, betas=(0.5, 0.999), weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392990a-d54c-45d1-9679-5e842107039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train following the GAN network process\n",
    "from tqdm import trange\n",
    "\n",
    "# read in the training parameters\n",
    "stylize_params = config['stylize_params']\n",
    "# check the dense motion module\n",
    "if generator.dense_motion_network is None:\n",
    "    print(\"Error: dense motion network doesn't exist!\")\n",
    "dm_network = generator.dense_motion_network # this model is used for extracting motion features\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# loop over\n",
    "for epoch in trange(stylize_params['num_epochs']):\n",
    "    for x in dataloader:\n",
    "        # first get the key points for both source and driving\n",
    "        x['source'] = x['source'].to(device)    \n",
    "        kp_source = kp_detector(x['source'])\n",
    "        x['driving'] = x['driving'].to(device)\n",
    "        kp_driving = kp_detector(x['driving'])\n",
    "\n",
    "        # second pass through the motion predictor\n",
    "        # plan A: get sparse motion as training data\n",
    "        if dm_network.scale_factor != 1:\n",
    "            src_image = dm_network.down(x['source'])\n",
    "        bs, _, h, w = src_image.shape\n",
    "        sparse_motion = dm_network.create_sparse_motions(src_image, kp_driving, kp_source)\n",
    "        # here we don't need the last key point, which is a identity grid layer added by users\n",
    "        sparse_motion = sparse_motion[:, :-1, :, :, :] # The shape should be: torch.Size([10, 10, 64, 64, 2])\n",
    "        orig_shape = sparse_motion.shape # record the orginal shape\n",
    "        sparse_motion = sparse_motion.view((orig_shape[0], -1, orig_shape[2], orig_shape[3]))\n",
    "        \n",
    "        # pass in stylizer network\n",
    "        loss_log = {}\n",
    "        # calculate losses for stylizer\n",
    "        stylized_dict = stylizer(sparse_motion.detach())\n",
    "        loss_values = {}\n",
    "        # for now, using MSE compared with original motion field, and GAN stylizer loss\n",
    "        mse_loss = F.mse_loss(sparse_motion, stylized_dict['prediction'])\n",
    "        loss_values['mse'] = mse_loss * stylize_params['loss_weights']['mse'] \n",
    "        # GAN stylizer loss\n",
    "        # TODO: add real face data, for now just running through\n",
    "        feature_real, discrim_real = styDiscrim(sparse_motion.detach()) # TODO: this feature should never be used\n",
    "        feature_gene, discrim_gene = styDiscrim(stylized_dict['prediction'])\n",
    "        gan_loss = ((1 - discrim_gene) ** 2).mean()\n",
    "        loss_values['gan'] = gan_loss * stylize_params['loss_weights']['gan'] \n",
    "        # TODO: consider add feature matching loss\n",
    "        if stylize_params['loss_weights']['match'] != 0:\n",
    "            value_total = 0\n",
    "            for i, (a, b) in enumerate(zip(feature_real, feature_gene)):\n",
    "                # calculate feature matching loss for each scale\n",
    "                value = torch.abs(a - b).mean()\n",
    "                value_total += value\n",
    "            loss_values['match'] = value_total * stylize_params['loss_weights']['match']\n",
    "        # Now combine all loss values for stylizer, update stylizer\n",
    "        loss_log.update(loss_values) # this is for logging\n",
    "        loss = sum([val.mean() for val in loss_values.values()])\n",
    "        loss.backward()\n",
    "        optimizer_stylizer.step()\n",
    "        optimizer_stylizer.zero_grad()\n",
    "        \n",
    "        # Now deal with discriminator training\n",
    "        loss_values = {}\n",
    "        optimizer_styDiscrim.zero_grad()\n",
    "        _, discrim_real = styDiscrim(sparse_motion.detach()) # TODO: this feature should never be used\n",
    "        _, discrim_gene = styDiscrim(stylized_dict['prediction'].detach())\n",
    "        discrim_loss = (1 - discrim_real) ** 2 + discrim_gene ** 2\n",
    "        loss_values['discrim'] = discrim_loss.mean() * stylize_params['loss_weights']['discrim']\n",
    "        # combine losses for stylizer discriminator, update discriminator\n",
    "        loss_log.update(loss_values) # this is for logging\n",
    "        loss = sum([val.mean() for val in loss_values.values()])\n",
    "        loss.backward()\n",
    "        optimizer_styDiscrim.step()\n",
    "        optimizer_styDiscrim.zero_grad()\n",
    "        \n",
    "        # End of an epoch\n",
    "        # TODO: logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3fd8f-c862-4241-8dc5-3cd0680fd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216db98-5594-40a8-ab82-34723c144d7f",
   "metadata": {},
   "source": [
    "## Model Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c22bd-4607-4d95-a90f-d6706529bdb1",
   "metadata": {},
   "source": [
    "## Code snippets part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814d334-3a9a-4ebb-a67f-ca0a1e27b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 4, 4\n",
    "x = torch.arange(w).type(torch.float)\n",
    "y = torch.arange(h).type(torch.float)\n",
    "\n",
    "x = (2 * (x / (w - 1)) - 1)\n",
    "y = (2 * (y / (h - 1)) - 1)\n",
    "\n",
    "yy = y.view(-1, 1).repeat(1, w)\n",
    "xx = x.view(1, -1).repeat(h, 1)\n",
    "\n",
    "meshed = torch.cat([xx.unsqueeze_(2), yy.unsqueeze_(2)], 2)\n",
    "\n",
    "print(meshed.shape)\n",
    "\n",
    "meshed = meshed.view(1, 1, h, w, 2)\n",
    "meshed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ebb3c-945a-4546-9ca9-e4d9825068ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
